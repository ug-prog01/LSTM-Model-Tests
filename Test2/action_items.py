# AUTHOR: UTKARSH MISHRA
"""This program is used to analyze the transcript generated by the analysis.py program and generate
the action items present in the given conversation and assign them accordingly to the person reponsible"""
"""To use this file independently the transcript of the conversation with tags must be
present in 'conv_with_tags.txt' file and the read, write persmissions are required to read the 'conv_with_tags.txt' file
and write the results in the 'action.txt' file."""

from keras.models import model_from_json
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import numpy as np
import os
import pickle
import pandas as pd
from nltk import sent_tokenize
from nltk.tokenize import PunktSentenceTokenizer
import re
from clean import clean_text, clean_text_one
import sys
import spacy
from pprint import pprint
nlp = spacy.load('en_core_web_sm')

action_file = 'action.txt'
filename = 'conv_with_tags.txt'

"""This function takes in two parameters viz. the file in which the conversation transcript is present and 
the model which will be used to predict the type of sentence present in the conversation. It predicts the type
of sentence i.e. action sentence or not, using the specified model and performs subject extraction on the 
action sentences. The subject and the action item is then stored locally in the file 'action.txt' and then
subsequently mailed to the required parties.
"""

def process(filename):

    final_sents = []
    final_sents_with_tags = []
    subjects = []
    action_items = []
    tags = []
    sentences = []
    with open(filename) as h:
        data = h.read()
        data = data.splitlines()        
        for i in range(len(data)):
            sep = data[i].split(':')
            tags.append(sep[0])
            # print(sep[0])
            sentences.append(sep[1])
        # print(sentences)
        text = clean_text(sentences)
        print(text)

    Y = []

    for phrase in text:
        tokens = tokenizer.texts_to_sequences([phrase])
        tokens = pad_sequences(tokens, maxlen=4000)
        prediction = loaded_model.predict(np.array(tokens))
        i,j = np.where(prediction == prediction.max()) 
        i = int(i)
        j = int(j)
        # print(prediction)
        total_possible_outcomes = ["1yes","2no"]
        Y.append(int(j))
        # print("String: ", phrase, "\tResult:",total_possible_outcomes[j])

    for sent, y in zip(text, Y):
        if(not y):
            doc = nlp(sent)
            sub_toks = [tok for tok in doc if (tok.dep_ == "nsubj") ]
            # print(sub_toks) 
            you = [i for i in sub_toks if str(i) == 'you']
            if(len(you) >= 1):
                final_sents_with_tags.append([you, sent])
            action_items.append([[(X.text, X.label_) for X in doc.ents], str(sent)])
    # print(action_items)

    subjects = []

    for item in action_items:
        try:
            if(item[0][0][1] != 'PLACE_HOLDER'):
                subjects.append(item[0][0][0])

        except:
            pass

    # print(subjects)
    cleaned_text = []
    k = 0
    for i in range(len(tags)):
        clean_sent = clean_text_one(sentences[i])
        try:
            # print(str(action_items[k][1]), str(clean_sent))
            if(str(action_items[k][1]) == str(clean_sent)):
                k += 1
                cleaned_text.append([str(clean_sent), tags[i+2]])
        except:
            pass
    
    
    tasks = []
    for clean in cleaned_text:
        for subject in subjects:
            if(subject in clean[0]):
                tasks.append([subject, clean])

    with open(action_file, 'w') as a:
        pass
    for task in tasks:
        with open(action_file, 'a+') as actions:
            string = str(task[0]) + ': ' + str(task[1][0]) +'\n'
            actions.write(string)


"""This program uses a LSTM model to predict the type of sentence that is present in the given transcript.
The model used can be changed by specifying the model path in the following function. The output of the model
used is in binary format i.e. either it is a action sentence or it isn't. If another model is used modify the 
process, function present above, accordingly.
"""


if __name__ == '__main__':
    
    with open('apollo145.json', 'r') as json_file:
        loaded_model_json = json_file.read()

    loaded_model = model_from_json(loaded_model_json)

    loaded_model.load_weights("model145.h5")

    with open('tokenizer145.pickle', 'rb') as handle:
        tokenizer = pickle.load(handle)

    process(filename)